{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wide parameter space searches and Machine learning for CW detection\n",
        "Joe Bayley\n",
        "\n",
        "Department of Physics and Astronomy\n",
        "\n",
        "Univeristy of Glasgow"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Searches for continuous gravitational waves come in a number of forms, generally targeted, directed and all-sky depending on the prior knowledge of the source. The following tutoral will be focussing on all-sky searches where there is no known prior knowledge of the source.\n",
        "\n",
        "There are two main parts to this tutorial:\n",
        " * SOAP - the core SOAP algorithm and how to apply it to LIGO/Virgo/Kagra data\n",
        " * ML - how to apply machine learning to the same data-set (in this case for detection, in reality it is used more for line-vetoing)\n",
        "\n",
        " You can open this notebook in colab, which has some small amount of access to GPUs for free:\n",
        " \n",
        " [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jcbayley/cwworkshop/blob/main/cwworkshop_soap.ipynb)\n",
        "\n",
        " For a lot of this notebook we will use a reduced sized data set to make the quantitiy more managable, but the techniques can equally be applied to the full dataset.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing and importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DNVFGgnpd13"
      },
      "outputs": [],
      "source": [
        "!pip install soapcw\n",
        "!pip install corner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O42RQn5RpMG_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import soapcw\n",
        "from soapcw import cw\n",
        "import h5py\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import corner"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_data(url, filename):\n",
        "    \"\"\"Fetch data from a url and save it to given file\"\"\"\n",
        "    if not os.path.isfile(filename):\n",
        "        import urllib\n",
        "        urllib.request.urlretrieve(url, filename=filename)\n",
        "    else:\n",
        "        print(\"File already exists!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make some directories\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "os.makedirs(\"pretrained_model_config\", exist_ok=True)\n",
        "# download some training data (both even and odd categories)\n",
        "even_url = \"https://github.com/jcbayley/cwworkshop/raw/main/data/freq_100.0_106.1_3120_even.hdf5\"\n",
        "even_file = \"data/freq_100.0_106.1_3120_even.hdf5\"\n",
        "fetch_data(even_url, even_file)\n",
        "odd_url = \"https://github.com/jcbayley/cwworkshop/raw/main/data/freq_100.0_106.1_3120_odd.hdf5\"\n",
        "odd_file = \"data/freq_100.0_106.1_3120_odd.hdf5\"\n",
        "fetch_data(odd_url, odd_file)\n",
        "# Now for configuration of the data and models\n",
        "config_url = \"https://github.com/jcbayley/cwworkshop/raw/main/pretrained_model_config/model.ini\"\n",
        "config_file = \"pretrained_model_config/model.ini\"\n",
        "fetch_data(config_url, config_file)\n",
        "# Now for some CNN model weights\n",
        "weight_url = \"https://github.com/jcbayley/cwworkshop/raw/main/pretrained_model_config/model_vitmapspectrogram_for_odd.pt\"\n",
        "weight_file = \"pretrained_model_config/model_vitmapspectrogram_for_odd.pt\"\n",
        "fetch_data(weight_url, weight_file)\n",
        "# Now for some of the parameters estimation weights\n",
        "nevillemodel_url = \"https://github.com/jcbayley/cwworkshop/raw/main/pretrained_model_config/neville_model.pt\"\n",
        "neville_file = \"pretrained_model_config/neville_model.pt\"\n",
        "fetch_data(nevillemodel_url, neville_file)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SOAP\n",
        "\n",
        "Documentation: https://joseph.bayley.docs.ligo.org/soapcw/viterbialgorithm.html \n",
        "\n",
        "SOAP (Snakes On A Plane) is a method to rapidly search for long duration signals in time-frequency spectrograms which do not follow any particular frequency evolution. This has the main goal of identifying signals that may be missed by traditional searches which use information on the expected signal to search for a signal.\n",
        "\n",
        "There are multiple components to the SOAP search:\n",
        "1. Initial frequency track identification (model agnostic)\n",
        "2. ML followup to penalise instrumental lines (some model dependence)\n",
        "3. Source parameter estimation from frequency tracks (model dependence)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vAShvRRcILLH"
      },
      "source": [
        "##  Data Generation\n",
        "Usage: https://joseph.bayley.docs.ligo.org/soapcw/usage/generate_cw_signal.html \n",
        "\n",
        "First off we will need to generate some data to search through, this will be a simple Gaussian noise time-series with a CW signal injected (here we actually simulate the power spectrum directly). \n",
        "\n",
        "We can use the SOAP package, which has a method to simulate a signal time-frequency power spectra. Whilst we are using Gaussian noise here, we can as easily input some real PSDs to simulate a more relistic signal or inject a signal into real data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp2CNTy3pzlb"
      },
      "outputs": [],
      "source": [
        "sig = cw.GenerateSignal()\n",
        "# define signal parameters\n",
        "sig.alpha = 1.7659              # right ascension\n",
        "sig.delta = 0.9                 # declination\n",
        "sig.cosi = 0.434                # cosine of the inclination\n",
        "sig.phi0 = 4.9115               # initial phase\n",
        "sig.psi = 0.6667                # polarisation angle\n",
        "sig.f = [100.06,-8.0e-16,0]     # frequency parameters [f, fdot, ...]\n",
        "sig.tref = 1238166483.0         # reference time \n",
        "#sig.h0 = 3e-24                 # can be used along with a noise floor value, but we'll just just SNR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yne7-P6oD3LF"
      },
      "outputs": [],
      "source": [
        "nsft = 48*200               # make 200 days of data (48, 1800s sfts = 1day)\n",
        "tstart = 1238166483.0       # arbritrary start time\n",
        "tsft = 1800.                # duration of 1 SFT\n",
        "flow = 100.0                # lower frequency bound of sub-band\n",
        "fhigh = 100.1               # upper frequency bound of sub-band\n",
        "snr = 160                   # signal to noise ratio of signal\n",
        "detectors = [\"H1\", \"L1\"]    # which detectors to simulate data for"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS3rtYI6D-Ic"
      },
      "outputs": [],
      "source": [
        "spect = sig.get_spectrogram(\n",
        "    tstart = tstart, \n",
        "    nsft=nsft,\n",
        "    tsft=tsft,\n",
        "    fmin=flow,\n",
        "    fmax=fhigh,\n",
        "    dets=detectors,\n",
        "    snr=snr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Spectrograms are usually summed over 1 day to remove the antenna pattern modulation and increase the SNR (assuming the signal stays within a single frequency bin for a day)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VQSfUdJD-vL"
      },
      "outputs": [],
      "source": [
        "spect.sum_sfts() # default number of summed SFTs is 48"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also compute the injected signal path to compare to our recovered signal later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# signal path in SFTs\n",
        "h1_pulsar_path = sig.get_pulsar_path(spect.epochs, \"H1\")\n",
        "# signal path in summed SFTs (taking mean over every 24 hours)\n",
        "h1_summed_pulsar_path = np.mean(np.split(h1_pulsar_path, int(len(h1_pulsar_path)/48)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "id": "1soRN3K4ERN4",
        "outputId": "19888dcd-b7aa-40e7-ce11-9fc2066f8f3d"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=2,figsize=(14,10))\n",
        "ax[0].imshow(spect.H1.norm_sft_power.T,aspect=\"auto\",origin=\"lower\",extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "ax[1].imshow(spect.H1.summed_norm_sft_power.T,aspect=\"auto\",origin=\"lower\",extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "ax[0].plot(spect.epochs, h1_pulsar_path, \"r\", label=\"pulsar path\")\n",
        "ax[0].set_xlabel(\"GPS time [s]\",fontsize=20)\n",
        "ax[0].set_ylabel(\"Frequency [Hz]\",fontsize=20)\n",
        "ax[1].set_xlabel(\"GPS time [s]\",fontsize=20)\n",
        "ax[1].set_ylabel(\"Frequency [Hz]\",fontsize=20)\n",
        "ax[0].legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transisiton matrix\n",
        "Documentation: https://joseph.bayley.docs.ligo.org/soapcw/transitionmatrix.html\n",
        "\n",
        "The transition matrix defines the constraints that are placed on the track as it iterates between one time step and the next. In this case we are using multiple detectors therefore there are three dimensions to the transition matrix. (up/center/down probability, geocenter to detector 1 probability, geocenter to detector 2 probability)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this transition matrix, we make it very unlikely for each detector to have a signal in neighbouring frequency bins i.e. we enforce the signal to be in the same bin in each detector. We also make it ever so slightly more likely for a signal to jump straight than to jump up or down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dmq5qWKrNf7_"
      },
      "outputs": [],
      "source": [
        "transition_matrix = soapcw.tools.transition_matrix_2d(1.1, 1e200,1e200)\n",
        "print(transition_matrix)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running SOAP"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The SOAP algorithm can then be run by inputting the two normalised and summed spectrograms and the transition matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj_tnL39H2g9"
      },
      "outputs": [],
      "source": [
        "soapout = soapcw.two_detector(transition_matrix, spect.H1.summed_norm_sft_power, spect.H1.summed_norm_sft_power)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can look at the output spectrograms, with the true signal path and the predicted signal path overlayed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=2,figsize=(14,10))\n",
        "half_day = 12*3600\n",
        "ax[0].imshow(spect.H1.summed_norm_sft_power.T,aspect=\"auto\",origin=\"lower\",extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "ax[1].imshow(spect.L1.summed_norm_sft_power.T,aspect=\"auto\",origin=\"lower\",extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "ax[0].plot(spect.epochs[::48] + half_day, spect.frequencies[soapout.vit_track1], color=\"C3\", marker=\"o\", ms=3, label=\"H1 viterbi path\")\n",
        "ax[1].plot(spect.epochs[::48] + half_day, spect.frequencies[soapout.vit_track2], color=\"C3\", marker=\"o\", ms=3, label=\"L1 viterbi path\")\n",
        "ax[0].plot(spect.epochs[::48] + half_day, h1_summed_pulsar_path, color=\"k\", label=\"Pulsar path\")\n",
        "ax[1].plot(spect.epochs[::48] + half_day, h1_summed_pulsar_path, color=\"k\", label=\"Pulsar path\")\n",
        "ax[1].set_xlabel(\"GPS time [s]\",fontsize=20)\n",
        "ax[0].set_ylabel(\"Frequency [Hz] (H1)\",fontsize=20)\n",
        "ax[1].set_ylabel(\"Frequency [Hz] (L1)\",fontsize=20)\n",
        "ax[0].legend()\n",
        "ax[1].legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also view the viterbi map. we take the log of the viterbi map probabilities here just to make the features more visible. The which reigons are where the \"probability\" of signal is zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "NqlQwp1_NbeS",
        "outputId": "e5d958f6-70c0-403e-e737-f1d985433fe6"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(14,5))\n",
        "img=ax.imshow(np.log(soapout.vitmap.T), origin=\"lower\", aspect=\"auto\", extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "cbar = fig.colorbar(img, ax=ax)\n",
        "cbar.set_label(\"Log normalised viterbi stat\", fontsize=20)\n",
        "ax.set_xlabel(\"GPS time [s]\",fontsize=20)\n",
        "ax.set_ylabel(\"Frequency [Hz]\",fontsize=20)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Line aware statistic"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Documentation: https://joseph.bayley.docs.ligo.org/soapcw/bayesianlineaware.html \n",
        "\n",
        "Usage: https://joseph.bayley.docs.ligo.org/soapcw/usage/generate_lookup_table.html"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Line aware statistic is designed to penalise instrumental lines compared to a signal model. This can only be used with two detectors and the penalisation comes from large differences in power between the detectors. \n",
        "\n",
        "More information about how this is derived can be found here: https://joseph.bayley.docs.ligo.org/soapcw/bayesianlineaware.html "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to define the ranges over which the spectrogram power is expected within a detector. Here we are using spectrograms summed over 1 day so the mean value of these would be 96 (mean of 2 for each SFT and 48 SFTs). Here the lookuptable is very course, only 10 points, in real searches there are many more values. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "powers = np.linspace(1,400,10) # we use a low resolution for speed in this example"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then generate the line aware statistic. The signal prior width refers to the width on the exponential prior on SNR for the signal model and the line prior width is the same for the line. Here we assume that instumental lines can have a much larger SNR. The noise line model ratio refers to the ratio of the probability of the noise model and the signal model. The parameters used here were optimised over the O3 observing run, however the sensitivity of a search is not that sensitive to these parameters over certain ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrgBj7Q2Nc4X"
      },
      "outputs": [],
      "source": [
        "line_aware = soapcw.line_aware_stat.gen_lookup_python.LineAwareStatistic(\n",
        "    powers,\n",
        "    ndet=2,\n",
        "    signal_prior_width=4.0,\n",
        "    line_prior_width=10.0,\n",
        "    noise_line_model_ratio=0.4\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "img = ax.imshow(np.log(line_aware.signoiseline),origin=\"lower\",extent=[powers.min(),powers.max(),powers.min(),powers.max()], cmap=\"cividis\")\n",
        "ax.set_xlabel(\"normalised spectogram power (det1)\")\n",
        "ax.set_ylabel(\"normalised spectogram power (det2)\")\n",
        "fig.colorbar(img, ax=ax, label=\"Output Line aware statistic\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can simulate an instrumental line by just adding some value to a set of bins at a fixed frequency, perhaps not realistic but quick and demonstrates the advantage of the statistic. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "line_H1_summed_power = copy.copy(spect.H1.summed_norm_sft_power)\n",
        "line_H1_summed_power[:,65:70] += 90"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then run soap, inputting the line aware statistic as a lookup table. There are two options as input, lookup_table_2det and lookup_table_1det, when running on real data both of these are included as there are times when only one detector is running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "soapout_line = soapcw.two_detector(transition_matrix, line_H1_summed_power, spect.L1.summed_norm_sft_power, lookup_table_2det=line_aware)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now view the outputs, it is clear that the area around the line is disfavoured and heavily penalised by the statistic. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=3,figsize=(14,10))\n",
        "ax[0].imshow(line_H1_summed_power.T,aspect=\"auto\",origin=\"lower\",extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "ax[1].imshow(spect.L1.summed_norm_sft_power.T,aspect=\"auto\",origin=\"lower\",extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "ax[0].plot(spect.epochs[::48] + half_day, spect.frequencies[soapout_line.vit_track1], color=\"r\", marker=\"o\", ms=2, label=\"H1 viterbi track\")\n",
        "ax[1].plot(spect.epochs[::48] + half_day, spect.frequencies[soapout_line.vit_track2], color=\"r\", marker=\"o\", ms=2, label=\"L1 viterbi track\")\n",
        "\n",
        "ax[0].plot(spect.epochs[::48] + half_day, h1_summed_pulsar_path, color=\"k\", label=\"Pulsar path\")\n",
        "ax[1].plot(spect.epochs[::48] + half_day, h1_summed_pulsar_path, color=\"k\", label=\"Pulsar path\")\n",
        "ax[2].imshow(np.log(soapout_line.vitmap.T),aspect=\"auto\",origin=\"lower\",extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "\n",
        "ax[2].set_xlabel(\"GPS time [s]\",fontsize=20)\n",
        "ax[2].set_ylabel(\"Frequency [Hz]\",fontsize=20)\n",
        "ax[0].legend()\n",
        "ax[1].legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m3bBgDFZGx_7"
      },
      "source": [
        "Generating useable outputs from the search involves running on many narrow bands of data. We can do this on an example set provided. The provided data contains data with and without injections.\n",
        "\n",
        "Noise: The noise is equivalent to a Gaussian noise time series, i.e. the power spectrum is a chi2 distribution with two degrees of freedom. \n",
        "\n",
        "Signal: The signal is injected with a given SNR, the square of which is used as the non centrality parameter for the noncentral chi2 distribution. The power is spread over multiple bins.\n",
        "\n",
        "There is a third set of data here which contains some instrumental artefacts to more closely simulate real data.\n",
        "\n",
        "Each of these datasets are a reduced set, so that we can run searches in a reasonable amount of time.\n",
        "The duration is 20 days (1800s SFTs) and it covers a 4Hz frequency range (100-106 Hz), where each band in 0.02 Hz wide. As the spectrograms are summed over 1 day, this leaves us with images that are 20x36. \n",
        "\n",
        "The injected signals are all very loud with integrated SNRs in the range 35->60 over 20 days. This will allow the network to learn something with a reduced number of training examples. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prior on Signals\n",
        "\n",
        "| name| symbol| range| info|\n",
        "|-----|-----|-----|-----|\n",
        "|right ascension | $\\alpha$| $[0, 2\\pi]$ | |\n",
        "|declination | $\\delta$| $[-\\pi/2, \\pi/2]$| |\n",
        "|cos inclination |$\\cos{\\iota}$| $[0,1]$| |\n",
        "|initial phase| $\\phi_0$ | $[0,2\\pi]$| |\n",
        "|polarisation | $\\psi$ | $[0, 2\\pi]$| |\n",
        "|frequency | $f$ | $[0.25 f_{\\rm{min}}, 0.75 f_{\\rm{min}}]$| place in middle half of narrowband|\n",
        "|spin down |$\\dot{f}$| $0$| |\n",
        "|Integrated snr | $\\rho$| [35, 60]| used to scale h0 value to noise floor|\n",
        "\n",
        "Data info\n",
        "\n",
        "| parameter| symbol| range| info|\n",
        "|-----|-----|-----|-----|\n",
        "|minimum frequency | $f_{min}$ | 100.0 Hz| |\n",
        "|maximum frequency | $f_{max}$| 106.0 Hz| |\n",
        "|sub-band width | | 0.02 Hz| 36 frequency bins|\n",
        "|SFT length | | 1800s| |\n",
        "|number summed SFTs|  | 48 | |\n",
        "|Duration | $T$ | 24 days| (912, 1800s SFTs)|\n",
        "|Bandwidth | $f_{\\rm{width}}$ | 0.02 Hz| 36, 1/1800 Hz bins|\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One thing you will notice here is the bands have been split into an \"even\" and \"odd\" category. Each sub-band is alternately put in to each of the categories. This is so that a separate machine learning model can be trained on each and tested on the opposite, this means we are never training on testing data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCNi8CYNH2sJ"
      },
      "outputs": [],
      "source": [
        "with h5py.File(even_file,\"r\") as f:\n",
        "    print(f.keys())\n",
        "    even_stats = np.array(f[\"stats\"])\n",
        "    even_imgs = np.transpose(np.array([np.array(f[\"H_imgs\"]), np.array(f[\"L_imgs\"]), np.array(f[\"vit_imgs\"])]), (1, 0, 2, 3))\n",
        "    even_labels = np.array(f[\"labels\"])\n",
        "    even_onehotlabels = torch.nn.functional.one_hot(torch.Tensor(even_labels).to(torch.int32).long(), 2).to(torch.float32)\n",
        "    #even_snrs = np.array(f[\"pars\"])[:,np.where(list(f[\"parnames\"]) == \"snr\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(np.shape(even_imgs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with h5py.File(odd_file,\"r\") as f:\n",
        "    print(f.keys())\n",
        "    odd_stats = np.array(f[\"stats\"])\n",
        "    odd_imgs = np.transpose(np.array([np.array(f[\"H_imgs\"]), np.array(f[\"L_imgs\"]), np.array(f[\"vit_imgs\"])]), (1, 0, 2, 3))\n",
        "    odd_labels = np.array(f[\"labels\"])\n",
        "    odd_onehotlabels = torch.nn.functional.one_hot(torch.Tensor(odd_labels).to(torch.int32).long(), 2).to(torch.float32)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can have a look at the data that is used in this example. We can see that this is a very small dataset with very load signal, this is to make the machine learning task later managable. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SFTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows = 2, ncols = 10, figsize = (14, 6))\n",
        "ind=3\n",
        "vmin,vmax = odd_imgs[:10, :2].min(), odd_imgs[:10, :2].max()\n",
        "for i in range(10):\n",
        "    ax[0, i].imshow(odd_imgs[i,0].T, cmap=\"cividis\",vmin=vmin, vmax=vmax)\n",
        "\n",
        "for i in range(10):\n",
        "    ax[1, i].imshow(odd_imgs[int(len(odd_imgs)/2) + i,0].T, cmap=\"cividis\",vmin=vmin, vmax=vmax)\n",
        "#ax.set_title(odd_labels[ind])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows = 2, ncols = 10, figsize = (14, 6))\n",
        "ind=3\n",
        "vmin,vmax = odd_imgs[:10, 2].min(), odd_imgs[:10, 2].max() -0.3\n",
        "for i in range(10):\n",
        "    img = ax[0, i].imshow(odd_imgs[i,2].T, cmap=\"cividis\",vmin=vmin, vmax=vmax)\n",
        "for i in range(10):\n",
        "    img = ax[1, i].imshow(odd_imgs[int(len(odd_imgs)/2) + i,2].T, cmap=\"cividis\",vmin=vmin, vmax=vmax)\n",
        "#ax.set_title(odd_labels[ind])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SOAP can be run on all examples in this folder, we can use the included files as the lookup tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "even_soapouts = np.zeros(len(even_imgs))\n",
        "for index in range(len(even_imgs)):\n",
        "    out = soapcw.two_detector(transition_matrix, even_imgs[index][0], even_imgs[index][1])\n",
        "    # soapcw.two_detector(transition_matrix, H_imgs[index], L_imgs[index], lookup_table)\n",
        "    even_soapouts[index] = out.max_end_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "odd_soapouts = np.zeros(len(odd_imgs))\n",
        "for index in range(len(odd_imgs)):\n",
        "    out = soapcw.two_detector(transition_matrix, odd_imgs[index][0], odd_imgs[index][1])\n",
        "    # soapcw.two_detector(transition_matrix, H_imgs[index], L_imgs[index], lookup_table)\n",
        "    odd_soapouts[index] = out.max_end_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(even_soapouts[:10])\n",
        "print(even_stats[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols=2, figsize=(14,6))\n",
        "bins = np.linspace(min(odd_stats), max(odd_stats), 40)\n",
        "hst = ax[0].hist(odd_stats[even_labels == 1], bins=bins, label=\"signal\", alpha=0.5)\n",
        "hst2 = ax[0].hist(odd_stats[even_labels == 0], bins=bins, label=\"noise\", alpha=0.5)\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"Viterbi statistic\")\n",
        "ax[0].set_ylabel(\"count\")\n",
        "ax[0].set_title(\"Line aware statistic\")\n",
        "\n",
        "bins = np.linspace(min(odd_soapouts), max(odd_soapouts), 30)\n",
        "hst = ax[1].hist(odd_soapouts[even_labels == 1], bins=bins, label=\"signal\", alpha=0.5)\n",
        "hst2 = ax[1].hist(odd_soapouts[even_labels == 0], bins=bins, label=\"noise\", alpha=0.5)\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"Viterbi statistic\")\n",
        "ax[1].set_ylabel(\"count\")\n",
        "ax[1].set_title(\"Summed statistic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far we have a dataset from which we want to learn if a signal is present. This falls into the category of binary classification, where we want the model to predict how probable it is that a signal is present. When dealing with binary classification we want to minimise the Binary cross entropy between the truth and the predicted output. This is defined by"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can define some parameters of out model here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "in_channels = 3                                             # [H,L,vitmap]\n",
        "outsize = 2                                                 # [noise_prob, signal_prob]\n",
        "n_epochs = 4000                                             # number times all data is seen\n",
        "learning_rate = 2e-4                                        # learning rate of the adam optimiser\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"   # which device to put the model and data on (i.e. CPU or GPU if available)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "you can print what gpus are available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(torch.cuda.device_count())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also check the size of our input spectrograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#imgshape\n",
        "print(odd_imgs.shape)\n",
        "print(even_imgs.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sequential model below is a simple way to build models, each layer follows the next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn = torch.nn.Sequential(\n",
        "    torch.nn.Conv2d(in_channels, 4, (7,7), padding=\"same\"),\n",
        "    torch.nn.MaxPool2d((1,2)),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Conv2d(4, 4, (3,3)),\n",
        "    torch.nn.MaxPool2d((1,2)),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.AdaptiveAvgPool2d((2,2)),\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.LazyLinear(16),\n",
        "    torch.nn.Dropout(0.4),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.LazyLinear(8),\n",
        "    torch.nn.Dropout(0.4),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.LazyLinear(outsize)\n",
        ").to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same model can also be created via SOAP, an example of the same model structure can be seen below.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_soap = soapcw.cnn.pytorch.models.CNN(\n",
        "    input_dim=(10,36),                          # the size of the input image\n",
        "    fc_layers=[16, 8, 2],                       # the size of the fully connected mlp layers \n",
        "    conv_layers=[(4, 7, 2, 1), (4, 3, 2, 1)],   # the convolutional layers (num_filters, filter_size, maxpool_size, stride)\n",
        "    inchannels=3, \n",
        "    avg_pool_size=(2,2),                        # the size of the average pooling layer \n",
        "    dropout=0.4,                                # the amount of dropout to use (default 0)\n",
        "    device=device).to(device)                   # put the model on the chosen gpu or cpu"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss and Optimiser"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's at this stage that we specify the loss function to use and what optimiser to use.\n",
        "\n",
        "The **optimiser** is the algorithm that is used to explore the parameter space of the network weights. In this example we're going to use `Adam` with a learning rate of 0.001 and the other parameters left as their default values. There a various different optimisers to choose from but Adam has proven to be realiable for a wide variety of problems and is a good place to start.\n",
        "\n",
        "We then need to define the function that will quantify the network's performance, the **loss function**. In this case we're using **Categorical Crossentropy**. This combined with the **Softmax layer** means the network will ouput a vector of probabilities for each samples where each probability corresponds to a particular class.\n",
        "\n",
        "For this binary (noise/signal) case it can written as:\n",
        "\n",
        "$$f(\\theta) = - \\sum_{i \\in S} log(\\theta_{i}^{S}) - \\sum_{i \\in N} log(\\theta_{i}^{N})$$\n",
        "\n",
        "where $\\theta_{i}^{S/N}$ is the predicted probability of class signal-noise (S) or noise-only (N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimiser = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_batch(model, optimiser, loss_function, data, labels, train=True):\n",
        "    \"\"\"\n",
        "    Compute the loss for one batch of training/validation data. If train=True update the model.\n",
        "    Args\n",
        "    -------\n",
        "    model: \n",
        "        pytorch model\n",
        "    optimiser:\n",
        "    loss_function:\n",
        "    data: Tensor\n",
        "    labels: Tensor\n",
        "    train: bool\n",
        "        if true updates the model weights and optimiser\n",
        "    \"\"\"\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "        \n",
        "    # zero the gradients so they do not grow with time\n",
        "    if train:\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "    # pass the data through the model to get two outputs (prob noise, prob signal)\n",
        "    outputs = model(data)\n",
        "    # computs the loss from our outputs and labels\n",
        "    loss = loss_function(outputs, labels)\n",
        "\n",
        "    if train:\n",
        "        # perform a backward pass computing the gradients within the network\n",
        "        loss.backward()\n",
        "        # take a step in the direction of these gradients\n",
        "        optimiser.step()\n",
        "\n",
        "    return loss.item()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a normalisation constant for the data. This is because the data is of order 100, whereas networks generally like numbers of order 1 as this can help mitigate problems with gradient explosions or vanishing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "norm_const = even_imgs.max()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Move all the data to the GPU (if the gpu is available) and normalise to this normalisation constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "even_imgs = torch.Tensor(even_imgs).to(device)/norm_const\n",
        "odd_imgs = torch.Tensor(odd_imgs).to(device)/norm_const\n",
        "even_onehotlabels = even_onehotlabels.to(device)\n",
        "odd_onehotlabels = odd_onehotlabels.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now write out simple training loop and train our model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tr_losses = np.zeros(n_epochs)\n",
        "val_losses = np.zeros(n_epochs)\n",
        "\n",
        "# define a random 200 indices for the validation data\n",
        "val_inds = np.random.uniform(0, len(odd_imgs), 200).astype(int)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # train a batch on all of the \"even\" data\n",
        "    tr_loss = train_batch(cnn, optimiser, loss_fn, even_imgs[:,:], even_onehotlabels, train=True)\n",
        "\n",
        "    # dont compute gradients for the test data \n",
        "    with torch.no_grad():\n",
        "        val_loss = train_batch(cnn, optimiser, loss_fn, odd_imgs[val_inds,:], odd_onehotlabels[val_inds], train=False)\n",
        "\n",
        "    # save the training and validation losses to array to look at later\n",
        "    tr_losses[epoch] = tr_loss\n",
        "    val_losses[epoch] = val_loss\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch: {epoch}, loss: {tr_loss}, val_loss: {val_loss}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can have a look at how the loss evolves during training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(tr_losses, label=\"Training loss\")\n",
        "ax.plot(val_losses, label=\"Validation Loss\")\n",
        "ax.set_yscale(\"log\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Loss\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now test this method on the \"odd\" bands to see how it performs on data it has not yet seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.eval()\n",
        "with torch.no_grad():\n",
        "    odd_outputs = cnn(torch.Tensor(odd_imgs[:,:])).cpu()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compute how well the cnn and SOAP performs by measuring the true positive rate at a false alarm of 1%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute 1% false alarm from noise only bands\n",
        "cnn_99per = sorted(odd_outputs[odd_labels==0][:,0])[int(0.99*len(odd_outputs[odd_labels==0]))]\n",
        "soap_99per = sorted(odd_soapouts[odd_labels==0])[int(0.99*len(odd_outputs[odd_labels==0]))]\n",
        "print(cnn_99per.numpy(), soap_99per)\n",
        "# compute fraction of signals that cross the 1% false alarm (efficiency)\n",
        "cnn_sigfrac = sum(odd_outputs[odd_labels==1][:,1] > cnn_99per)/len(odd_outputs[odd_labels==1])\n",
        "soap_sigfrac = sum(odd_soapouts[odd_labels==1] > soap_99per)/len(odd_outputs[odd_labels==1])\n",
        "print(\"cnn true positive rate: \", cnn_sigfrac.numpy())\n",
        "print(\"soap true positive rate: \", soap_sigfrac)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that it reaches close to the same sensitivity on this dataset with only a small amount of training and a small network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols = 2, figsize=(14,5))\n",
        "bins = np.linspace(odd_outputs[odd_labels==0][:,1].min(), odd_outputs[odd_labels==1][:,1].max(), 50)\n",
        "h11 = ax[0].hist(odd_outputs[odd_labels==0][:,1], bins=bins, alpha=0.5)\n",
        "h12 = ax[0].hist(odd_outputs[odd_labels==1][:,1], bins=bins, alpha=0.5)\n",
        "ax[0].axvline(cnn_99per, color=\"r\")\n",
        "ax[0].set_yscale(\"log\")\n",
        "h11 = ax[1].hist(torch.nn.functional.sigmoid(odd_outputs[odd_labels==0][:,1]), bins=30, alpha=0.5)\n",
        "h12 = ax[1].hist(torch.nn.functional.sigmoid(odd_outputs[odd_labels==1][:,1]), bins=30, alpha=0.5)\n",
        "ax[1].axvline(torch.nn.functional.sigmoid(cnn_99per), color=\"r\")\n",
        "ax[0].set_xlabel(\"CNN statistic\")\n",
        "ax[1].set_xlabel(\"CNN statistic (sigmoid)\")\n",
        "ax[0].set_ylabel(\"N bands\")\n",
        "ax[1].set_ylabel(\"N bands\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Challenge 🏆\n",
        "\n",
        "Can you modify the network above so that also makes some point predictions of some parameters? i.e. alpha, delta, f, fdot\n",
        "\n",
        "In the loaded h5py files there are some parameters associated with each of the signals, these have been loaded in already.\n",
        "\n",
        "You will need to write the model as a class to allow for both loss functions, an example of how to do this is here: https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html#define-the-class\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading a pretrained model\n",
        "Links to other pretrained models are in the ligo private pages here if needed, I've provided an example in this repo for those who do not have access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_soap, model_config = soapcw.cnn.pytorch.load_model_from_config(\n",
        "        config_file, \n",
        "        weight_file, \n",
        "        device=device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This can be tested on the above data (this is not an entirely fair test as the SNR distributions and data it was trained on are slightly different)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_soap.eval()\n",
        "with torch.no_grad():\n",
        "    s_odd_outputs = model_soap(torch.Tensor(odd_imgs)).cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols = 1, figsize=(8,5))\n",
        "bins = np.linspace(min(s_odd_outputs[:,1]), max(s_odd_outputs[:,1]), 40)\n",
        "h11 = ax.hist(s_odd_outputs[odd_labels==0][:,1], alpha=0.5, bins=bins)\n",
        "h12 = ax.hist(s_odd_outputs[odd_labels==1][:,1], alpha=0.5, bins=bins)\n",
        "ax.set_yscale(\"log\")\n",
        "ax.set_xlabel(\"Output statistic\")\n",
        "ax.set_ylabel(\"N bands\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can test this network on the data we simulated at the start of this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data = torch.unsqueeze(torch.Tensor(np.stack([spect.H1.summed_norm_sft_power.T, spect.L1.summed_norm_sft_power.T, soapout.vitmap.T])), 0)\n",
        "test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    s_odd_test = model_soap(test_data.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.nn.functional.sigmoid(s_odd_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neville "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neville is the parameter estimation part of SOAP to followup on interesting candidates. \n",
        "\n",
        "This is not yet integrated into the SOAP package, but will be within the next few months. Here I provide an example of a pretrained model and how it would be used. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neville_model, neville_config = soapcw.neville.load_models.load_model_from_config(config_file,neville_file, device=device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we have just padded the input with a fixed value. This is because the model above was trained on 362 days of data and above we have 200 days of data.\n",
        "\n",
        "We also normalise the inputs to be between 0 and 1. As the above model was trained on data in 0.1 Hz bands, the bin index went between 0 and 180, we divide by the 180."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pad and reshape to be input to model\n",
        "app_length = 362 - len(soapout.vit_track1)\n",
        "# this section adds a randomly wandering track at the end of the signal to pad the signal to be of same length that model was trained on.\n",
        "randtrack = [soapout.vit_track1[-1]]\n",
        "for i in range(app_length - 1):\n",
        "   randtrack.append(randtrack[-1] + np.random.randint(low=0,high=3, size=1) - 1)\n",
        "track_input = np.append(soapout.vit_track1, randtrack).astype(float)\n",
        "#track_input = copy.copy(soapout.vit_track2)\n",
        "track_input = np.expand_dims(np.expand_dims(track_input[:362], 0), 0)\n",
        "# also normalise the inputs to go between 0 and 1\n",
        "track_input = track_input/(180)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(track_input[0][0])\n",
        "ax.set_xlabel(\"epoch\")\n",
        "ax.set_ylabel(\"normalised frequency offset\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the output posterior samples\n",
        "- in this case there are a number of outputs (doppler posterior samples, transformed samples, track samples)\n",
        "- doppler samples are the samples on the $\\alpha$, $\\delta$, $f$, $\\dot{f}$ parameters\n",
        "- transformed samples can be ignored in this case as we are doing our own transformation\n",
        "- track samples are the samples from the probability that any track element is associated with a signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remember to turn off gradients and put the network into eval mode when testing\n",
        "neville_model.eval()\n",
        "with torch.no_grad():\n",
        "    samps = neville_model.test(torch.Tensor(track_input).to(torch.float32).to(device), freqs=torch.Tensor([flow,]).to(device), num_samples=5000)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As is common with machine learning problems, the parameters and data are scaled to be of order 1. Before training this model each parameters was scaled to be between 0 and 1, therefore we must rescale the parameters back up to their original units. \n",
        "\n",
        "For the sky we rescale to radians then convert from ecliptic coordinates to :\n",
        " - norm_longitude, norm_latitude - > $\\alpha$, $\\delta$\n",
        "\n",
        "The offset is rescaled to Hz and added to the base frequency of the band.\n",
        "\n",
        "The fdot is rescale to be of units Hz/s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trans_doppler_samps = np.array(soapcw.neville.tools.transform_normlonlat_alphadelta(flow, 0.1, samps[0][0,:,0], samps[0][0,:,1], samps[0][0,:,2], samps[0][0,:,3]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see and example of the predicted track, and also a representation of the track samples. For each track element we can take the mean of the associated samples and use this as a measure of the probability that a track element is consistent with the real signal. This is the color scale on the plot below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(13,5))\n",
        "im = ax.scatter(np.arange(len(track_input[0][0])), track_input[0][0], c = samps[2].mean(axis=1)[0], cmap=\"cividis\")\n",
        "ax.plot( (h1_summed_pulsar_path - flow)*10, color=\"r\", label=\"Pulsar path\")\n",
        "ax.set_xlabel(\"epoch\")\n",
        "ax.set_ylabel(\"normalised frequency offset\")\n",
        "cbar = fig.colorbar(im, ax=ax)\n",
        "cbar.set_label(\"Probability of track being signal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the true parameters based on the injected signal above.\n",
        "parameters = [sig.alpha, sig.delta, sig.f[0], sig.f[1]]\n",
        "print(parameters)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then produce a corner plot of the posterior samples on the four doppler parameters (here we have marginalised over each of the track element parameters). We can see from this example that it is consistent with the truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corner_fig = corner.corner(trans_doppler_samps.T, labels=[\"alpha\", \"delta\", \"f\", \"fdot\"], truths=parameters)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Soon training this network will be available via the SOAP package, however that functionality will be added within the next few months."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "soapcw",
      "language": "python",
      "name": "soapcw"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
