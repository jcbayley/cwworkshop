{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wide parameter space searches and Machine learning for CW detection\n",
        "Joe Bayley\n",
        "\n",
        "Department of Physics and Astronomy\n",
        "\n",
        "Univeristy of Glasgow"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Searches for continuous gravitational waves come in a number of forms, generally targeted, directed and all-sky depending on the prior knowledge of the source. The following tutoral will be focussing on all-sky searches where there is no known prior knowledge of the source.\n",
        "\n",
        "There are two main parts to this tutorial:\n",
        " * SOAP - the core SOAP algorithm and how to apply it to LIGO/Virgo/Kagra data\n",
        " * ML - how to apply machine learning to the same data-set (in this case for detection, in reality it is used more for line-vetoing)\n",
        "\n",
        " You can open this notebook in colab, which has some small amount of access to GPUs for free:\n",
        " \n",
        " [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jcbayley/cwworkshop/blob/main/cwworkshop_soap.ipynb)\n",
        "\n",
        " The datasets we will use throughout this tutoral are much smaller than we will realistically get, \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing and importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DNVFGgnpd13"
      },
      "outputs": [],
      "source": [
        "!pip install soapcw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O42RQn5RpMG_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import soapcw\n",
        "from soapcw import cw\n",
        "import h5py\n",
        "import torch\n",
        "import os\n",
        "import copy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_data(url, filename):\n",
        "    \"\"\"Fetch data from a url and save it to given file\"\"\"\n",
        "    if not os.path.isfile(filename):\n",
        "        import urllib\n",
        "        urllib.request.urlretrieve(url, filename=filename)\n",
        "    else:\n",
        "        print(\"File already exists!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs(\"data\", exist_ok=True)\n",
        "os.makedirs(\"pretrained_model_config\", exist_ok=True)\n",
        "even_url = \"https://github.com/jcbayley/cwworkshop/raw/main/data/freq_100.0_106.1_3120_even.hdf5\"\n",
        "even_file = \"data/freq_100.0_106.1_3120_even.hdf5\"\n",
        "fetch_data(even_url, even_file)\n",
        "# Now for the training data\n",
        "odd_url = \"https://github.com/jcbayley/cwworkshop/raw/main/data/freq_100.0_106.1_3120_odd.hdf5\"\n",
        "odd_file = \"data/freq_100.0_106.1_3120_odd.hdf5\"\n",
        "fetch_data(odd_url, odd_file)\n",
        "# Now for model weights\n",
        "weight_url = \"https://github.com/jcbayley/cwworkshop/raw/main/pretrained_model_config/model_vitmapspectrogram_for_odd.pt\"\n",
        "weight_file = \"pretrained_model_config/model_vitmapspectrogram_for_odd.pt\"\n",
        "fetch_data(weight_url, weight_file)\n",
        "# Now for  config\n",
        "config_url = \"https://github.com/jcbayley/cwworkshop/raw/main/pretrained_model_config/pre_model.ini\"\n",
        "config_file = \"pretrained_model_config/pre_model.ini\"\n",
        "fetch_data(config_url, config_file)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SOAP\n",
        "\n",
        "Documentation: https://joseph.bayley.docs.ligo.org/soapcw/viterbialgorithm.html \n",
        "\n",
        "SOAP (Snakes On A Plane) is a method to rapidly search for long duration signals in time-frequency spectrograms which do not follow any particular frequency evolution. This has the main goal of identifying signals that may be missed by traditional searches which use information on the expected signal to search for a signal.\n",
        "\n",
        "There are multiple components to the SOAP search:\n",
        "1. Initial frequency track identification (model agnostic)\n",
        "2. ML followup to penalise instrumental lines (some model dependence)\n",
        "3. Source parameter estimation from frequency tracks (model dependence)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vAShvRRcILLH"
      },
      "source": [
        "##  Data Generation\n",
        "Usage: https://joseph.bayley.docs.ligo.org/soapcw/usage/generate_cw_signal.html \n",
        "\n",
        "The SOAP package also has the ability to simulate spectrograms (i.e. time-frequency power spectra) of a CW signal. These are by default injected assuming the signal time-series is in stationary Gaussian noise, however, the PSD for each SFT can be changed and a signal can also be injected into real data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp2CNTy3pzlb"
      },
      "outputs": [],
      "source": [
        "sig = cw.GenerateSignal()\n",
        "# define signal parameters\n",
        "sig.alpha = 3.310726752188296\n",
        "sig.delta = -0.8824241920781501\n",
        "sig.cosi = -0.63086\n",
        "sig.phi0 = 4.007\n",
        "sig.psi = 0.52563\n",
        "sig.f = [100.02,-1e-17,0]\n",
        "sig.tref = 946339148.816094\n",
        "#sig.h0 = 3e-24   #can be used along with a noise floor value, but we'll just just SNR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yne7-P6oD3LF"
      },
      "outputs": [],
      "source": [
        "nsft, tstart, tsft, flow, fhigh = 48*200, 931042949, 1800., 100.0,100.1\n",
        "snr = 70"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS3rtYI6D-Ic"
      },
      "outputs": [],
      "source": [
        "spect = sig.get_spectrogram(\n",
        "    tstart = tstart, \n",
        "    nsft=nsft,\n",
        "    tsft=tsft,\n",
        "    fmin=flow,\n",
        "    fmax=fhigh,\n",
        "    dets=[\"H1\", \"L1\"],\n",
        "    snr=snr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Spectrograms are usually summed over 1 day to remove the antenna pattern modulation and increase the SNR (assuming the signal stays within a single frequency bin for a day)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VQSfUdJD-vL"
      },
      "outputs": [],
      "source": [
        "spect.sum_sfts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h1_pulsar_path = sig.get_pulsar_path(spect.epochs, \"H1\")\n",
        "h1_summed_pulsar_path = np.mean(np.split(h1_pulsar_path, int(len(h1_pulsar_path)/48)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "id": "1soRN3K4ERN4",
        "outputId": "19888dcd-b7aa-40e7-ce11-9fc2066f8f3d"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=2,figsize=(14,10))\n",
        "ax[0].imshow(spect.H1.norm_sft_power.T,aspect=\"auto\",origin=\"lower\",extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "ax[1].imshow(spect.H1.summed_norm_sft_power.T,aspect=\"auto\",origin=\"lower\",extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "ax[0].plot(spect.epochs, h1_pulsar_path, \"r\", label=\"pulsar path\")\n",
        "ax[0].set_xlabel(\"GPS time [s]\",fontsize=20)\n",
        "ax[0].set_ylabel(\"Frequency [Hz]\",fontsize=20)\n",
        "ax[1].set_xlabel(\"GPS time [s]\",fontsize=20)\n",
        "ax[1].set_ylabel(\"Frequency [Hz]\",fontsize=20)\n",
        "ax[0].legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transisiton matrix\n",
        "Documentation: https://joseph.bayley.docs.ligo.org/soapcw/transitionmatrix.html\n",
        "\n",
        "The transition matrix defines the constraints that are placed on the track as it iterates between one time step and the next. In this case we are using multiple detectors therefore there are three components to the transition matrix. (up/down probability, geocenter to detector 1 probability, geocenter to detector 2 probability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dmq5qWKrNf7_"
      },
      "outputs": [],
      "source": [
        "transition_matrix = soapcw.tools.transition_matrix_2d(1.1, 1e200,1e200)\n",
        "print(transition_matrix)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The SOAP algorithm can then be run by inputting the two normalised and summed spectrograms and the transition matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj_tnL39H2g9"
      },
      "outputs": [],
      "source": [
        "soapout = soapcw.two_detector(transition_matrix, spect.H1.summed_norm_sft_power, spect.H1.summed_norm_sft_power)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=2,figsize=(14,10))\n",
        "half_day = 12*3600\n",
        "ax[0].imshow(spect.H1.summed_norm_sft_power.T,aspect=\"auto\",origin=\"lower\",extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "ax[1].imshow(spect.L1.summed_norm_sft_power.T,aspect=\"auto\",origin=\"lower\",extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "ax[0].plot(spect.epochs[::48] + half_day, spect.frequencies[soapout.vit_track1], color=\"C3\", marker=\"o\", ms=3, label=\"H1 viterbi path\")\n",
        "ax[1].plot(spect.epochs[::48] + half_day, spect.frequencies[soapout.vit_track2], color=\"C3\", marker=\"o\", ms=3, label=\"L1 viterbi path\")\n",
        "ax[0].plot(spect.epochs[::48] + half_day, h1_summed_pulsar_path, color=\"k\", label=\"Pulsar path\")\n",
        "ax[1].plot(spect.epochs[::48] + half_day, h1_summed_pulsar_path, color=\"k\", label=\"Pulsar path\")\n",
        "ax[1].set_xlabel(\"GPS time [s]\",fontsize=20)\n",
        "ax[0].set_ylabel(\"Frequency [Hz] (H1)\",fontsize=20)\n",
        "ax[1].set_ylabel(\"Frequency [Hz] (L1)\",fontsize=20)\n",
        "ax[0].legend()\n",
        "ax[1].legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "NqlQwp1_NbeS",
        "outputId": "e5d958f6-70c0-403e-e737-f1d985433fe6"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(14,5))\n",
        "img=ax.imshow(np.log(soapout.vitmap.T), origin=\"lower\", aspect=\"auto\", extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "cbar = fig.colorbar(img, ax=ax)\n",
        "cbar.set_label(\"Log normalised viterbi stat\", fontsize=20)\n",
        "ax.set_xlabel(\"GPS time [s]\",fontsize=20)\n",
        "ax.set_ylabel(\"Frequency [Hz]\",fontsize=20)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Line aware statistic"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Documentation: https://joseph.bayley.docs.ligo.org/soapcw/bayesianlineaware.html \n",
        "\n",
        "Usage: https://joseph.bayley.docs.ligo.org/soapcw/usage/generate_lookup_table.html"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Line aware statistic is designed to penalise instrumental lines compared to a signal model. This can only be used with two detectors and the penalisation comes from large differences in power between the detectors. \n",
        "\n",
        "More information about how this is derived can be found here: https://joseph.bayley.docs.ligo.org/soapcw/bayesianlineaware.html "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "powers = np.linspace(1,400,10) # we use a low resolution for speed in this example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrgBj7Q2Nc4X"
      },
      "outputs": [],
      "source": [
        "line_aware = soapcw.line_aware_stat.gen_lookup_python.LineAwareStatistic(\n",
        "    powers,\n",
        "    ndet=2,\n",
        "    signal_prior_width=4.0,\n",
        "    line_prior_width=10.0,\n",
        "    noise_line_model_ratio=0.4\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "img = ax.imshow(np.log(line_aware.signoiseline),origin=\"lower\",extent=[powers.min(),powers.max(),powers.min(),powers.max()], cmap=\"cividis\")\n",
        "ax.set_xlabel(\"normalised spectogram power (det1)\")\n",
        "ax.set_ylabel(\"normalised spectogram power (det2)\")\n",
        "fig.colorbar(img, ax=ax, label=\"Output Line aware statistic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "line_H1_summed_power = copy.copy(spect.H1.summed_norm_sft_power)\n",
        "line_H1_summed_power[:,60:70] += 400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "soapout_line = soapcw.two_detector(transition_matrix, line_H1_summed_power, spect.L1.summed_norm_sft_power, lookup_table_2det=line_aware)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=3,figsize=(14,10))\n",
        "ax[0].imshow(line_H1_summed_power.T,aspect=\"auto\",origin=\"lower\",extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "ax[1].imshow(spect.L1.summed_norm_sft_power.T,aspect=\"auto\",origin=\"lower\",extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "ax[0].plot(spect.epochs[::48] + half_day, spect.frequencies[soapout_line.vit_track1], color=\"r\", marker=\"o\", ms=2, label=\"H1 viterbi track\")\n",
        "ax[1].plot(spect.epochs[::48] + half_day, spect.frequencies[soapout_line.vit_track2], color=\"r\", marker=\"o\", ms=2, label=\"L1 viterbi track\")\n",
        "\n",
        "ax[0].plot(spect.epochs[::48] + half_day, h1_summed_pulsar_path, color=\"k\", label=\"Pulsar path\")\n",
        "ax[1].plot(spect.epochs[::48] + half_day, h1_summed_pulsar_path, color=\"k\", label=\"Pulsar path\")\n",
        "ax[2].imshow(np.log(soapout_line.vitmap.T),aspect=\"auto\",origin=\"lower\",extent=[spect.epochs.min(),spect.epochs.max(),spect.frequencies.min(),spect.frequencies.max()],cmap=\"cividis\", interpolation=\"none\")\n",
        "\n",
        "ax[2].set_xlabel(\"GPS time [s]\",fontsize=20)\n",
        "ax[2].set_ylabel(\"Frequency [Hz]\",fontsize=20)\n",
        "ax[0].legend()\n",
        "ax[1].legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m3bBgDFZGx_7"
      },
      "source": [
        "Generating useable outputs from the search involves running on many narrow bands of data. We can do this on an example set provided. The provided data contains data with and without injections.\n",
        "\n",
        "Noise: The noise is equivalent to a Gaussian noise time series, i.e. the power spectrum is a chi2 distribution with two degrees of freedom. \n",
        "\n",
        "Signal: The signal is injected with a given SNR, the square of which is used as the non centrality parameter for the noncentral chi2 distribution. The power is spread over multiple bins.\n",
        "\n",
        "There is a third set of data here which contains some instrumental artefacts to more closely simulate real data.\n",
        "\n",
        "Each of these datasets are a reduced set, so that we can run searches in a reasonable amount of time.\n",
        "The duration is 20 days (1800s SFTs) and it covers a 4Hz frequency range (100-106 Hz), where each band in 0.02 Hz wide. As the spectrograms are summed over 1 day, this leaves us with images that are 20x36. \n",
        "\n",
        "The injected signals are all very loud with integrated SNRs in the range 30->40 over 10 days. This will allow the network to learn something with a reduced number of training examples. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prior on Signals\n",
        "\n",
        "| name| symbol| range| info|\n",
        "|-----|-----|-----|-----|\n",
        "|right ascension | $\\alpha$| $[0, 2\\pi]$ | |\n",
        "|declination | $\\delta$| $[-\\pi/2, \\pi/2]$| |\n",
        "|cos inclination |$\\cos{\\iota}$| $[0,1]$| |\n",
        "|initial phase| $\\phi_0$ | $[0,2\\pi]$| |\n",
        "|polarisation | $\\psi$ | $[0, 2\\pi]$| |\n",
        "|frequency | $f$ | $[0.25 f_{\\rm{min}}, 0.75 f_{\\rm{min}}]$| place in middle half of narrowband|\n",
        "|spin down |$\\dot{f}$| $0$| |\n",
        "|Integrated snr | $\\rho$| [35, 60]| used to scale the h0 to noise floor|\n",
        "\n",
        "Data info\n",
        "\n",
        "| parameter| symbol| range| info|\n",
        "|-----|-----|-----|-----|\n",
        "|minimum frequency | $f_{min}$ | 100.0 Hz| |\n",
        "|maximum frequency | $f_{max}$| 106.0 Hz| |\n",
        "|sub-band width | | 0.02 Hz| 36 frequency bins|\n",
        "|SFT length | | 1800s| |\n",
        "|nuber summed SFTs|  | 48 | |\n",
        "|Duration | $T$ | 24 days| (912, 1800s SFTs)|\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One thing you will notice here is the bands have been split into an \"even\" and \"odd\" category. Each sub-band is alternately put in to each of the categories. This is so that a separate machine learning model can be trained on each and tested on the opposite, this means we are never training on testing data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCNi8CYNH2sJ"
      },
      "outputs": [],
      "source": [
        "with h5py.File(even_file,\"r\") as f:\n",
        "    print(f.keys())\n",
        "    even_stats = np.array(f[\"stats\"])\n",
        "    even_imgs = np.transpose(np.array([np.array(f[\"H_imgs\"]), np.array(f[\"L_imgs\"]), np.array(f[\"vit_imgs\"])]), (1, 0, 2, 3))\n",
        "    even_labels = np.array(f[\"labels\"])\n",
        "    even_onehotlabels = torch.nn.functional.one_hot(torch.Tensor(even_labels).to(torch.int32).long(), 2).to(torch.float32)\n",
        "    #even_snrs = np.array(f[\"pars\"])[:,np.where(list(f[\"parnames\"]) == \"snr\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(np.shape(even_imgs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with h5py.File(odd_file,\"r\") as f:\n",
        "    print(f.keys())\n",
        "    odd_stats = np.array(f[\"stats\"])\n",
        "    odd_imgs = np.transpose(np.array([np.array(f[\"H_imgs\"]), np.array(f[\"L_imgs\"]), np.array(f[\"vit_imgs\"])]), (1, 0, 2, 3))\n",
        "    odd_labels = np.array(f[\"labels\"])\n",
        "    odd_onehotlabels = torch.nn.functional.one_hot(torch.Tensor(odd_labels).to(torch.int32).long(), 2).to(torch.float32)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can have a look at the data that is used in this example. We can see that this is a very small dataset with very load signal, this is to make the machine learning task later managable. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SFTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows = 2, ncols = 10, figsize = (14, 6))\n",
        "ind=3\n",
        "vmin,vmax = odd_imgs[:10, :2].min(), odd_imgs[:10, :2].max()\n",
        "for i in range(10):\n",
        "    ax[0, i].imshow(odd_imgs[i,0].T, cmap=\"cividis\",vmin=vmin, vmax=vmax)\n",
        "\n",
        "for i in range(10):\n",
        "    ax[1, i].imshow(odd_imgs[int(len(odd_imgs)/2) + i,0].T, cmap=\"cividis\",vmin=vmin, vmax=vmax)\n",
        "#ax.set_title(odd_labels[ind])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows = 2, ncols = 10, figsize = (14, 6))\n",
        "ind=3\n",
        "vmin,vmax = odd_imgs[:10, 2].min(), odd_imgs[:10, 2].max() -0.6\n",
        "for i in range(10):\n",
        "    img = ax[0, i].imshow(odd_imgs[i,2].T, cmap=\"cividis\",vmin=vmin, vmax=vmax)\n",
        "for i in range(10):\n",
        "    img = ax[1, i].imshow(odd_imgs[int(len(odd_imgs)/2) + i,2].T, cmap=\"cividis\",vmin=vmin, vmax=vmax)\n",
        "#ax.set_title(odd_labels[ind])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SOAP can be run on all examples in this folder, we can use the included files as the lookup tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "even_soapouts = np.zeros(len(even_imgs))\n",
        "for index in range(len(even_imgs)):\n",
        "    out = soapcw.two_detector(transition_matrix, even_imgs[index][0], even_imgs[index][1])\n",
        "    # soapcw.two_detector(transition_matrix, H_imgs[index], L_imgs[index], lookup_table)\n",
        "    even_soapouts[index] = out.max_end_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "odd_soapouts = np.zeros(len(odd_imgs))\n",
        "for index in range(len(odd_imgs)):\n",
        "    out = soapcw.two_detector(transition_matrix, odd_imgs[index][0], odd_imgs[index][1])\n",
        "    # soapcw.two_detector(transition_matrix, H_imgs[index], L_imgs[index], lookup_table)\n",
        "    odd_soapouts[index] = out.max_end_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(even_soapouts[:10])\n",
        "print(even_stats[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols=2, figsize=(14,6))\n",
        "bins = np.linspace(min(odd_stats), max(odd_stats), 40)\n",
        "hst = ax[0].hist(odd_stats[even_labels == 1], bins=bins, label=\"signal\", alpha=0.5)\n",
        "hst2 = ax[0].hist(odd_stats[even_labels == 0], bins=bins, label=\"noise\", alpha=0.5)\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"Viterbi statistic\")\n",
        "ax[0].set_ylabel(\"count\")\n",
        "ax[0].set_title(\"Line aware statistic\")\n",
        "\n",
        "bins = np.linspace(min(odd_soapouts), max(odd_soapouts), 30)\n",
        "hst = ax[1].hist(odd_soapouts[even_labels == 1], bins=bins, label=\"signal\", alpha=0.5)\n",
        "hst2 = ax[1].hist(odd_soapouts[even_labels == 0], bins=bins, label=\"noise\", alpha=0.5)\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"Viterbi statistic\")\n",
        "ax[1].set_ylabel(\"count\")\n",
        "ax[1].set_title(\"Summed statistic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far we have a dataset from which we want to learn if a signal is present. This falls into the category of binary classification, where we want the model to predict how probable it is that a signal is present. When dealing with binary classification we want to minimise the Binary cross entropy between the truth and the predicted output. This is defined by"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can define some parameters of out model here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "in_channels = 3                                             # [H,L,vitmap]\n",
        "outsize = 2                                                 # [noise_prob, signal_prob]\n",
        "n_epochs = 4000                                             # number times all data is seen\n",
        "learning_rate = 4e-4                                        # learning rate of the adam optimiser\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"   # which device to put the model and data on (i.e. CPU or GPU if available)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "you can print what gpus are available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(torch.cuda.device_count())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also check the size of our input spectrograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#imgshape\n",
        "print(odd_imgs.shape)\n",
        "print(even_imgs.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sequential model below is a simple way to build models, each layer follows the next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn = torch.nn.Sequential(\n",
        "    torch.nn.Conv2d(in_channels, 4, (7,7), padding=\"same\"),\n",
        "    torch.nn.MaxPool2d((1,2)),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Conv2d(4, 4, (3,3)),\n",
        "    torch.nn.MaxPool2d((1,2)),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.AdaptiveAvgPool2d((2,2)),\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.LazyLinear(16),\n",
        "    torch.nn.Dropout(0.4),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.LazyLinear(8),\n",
        "    torch.nn.Dropout(0.4),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.LazyLinear(outsize)\n",
        ").to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same model can also be created via the SOAP interface, an example of the same model structure can be seen below.     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_soap = soapcw.cnn.pytorch.models.CNN(\n",
        "    input_dim=(10,36),                          # the size of the input image\n",
        "    fc_layers=[16, 8, 2],                       # the size of the fully connected mlp layers \n",
        "    conv_layers=[(4, 7, 2, 1), (4, 3, 2, 1)],   # the convolutional layers (num_filters, filter_size, maxpool_size, stride)\n",
        "    inchannels=3, \n",
        "    avg_pool_size=(2,2),                        # the size of the average pooling layer \n",
        "    dropout=0.4,                                # the amount of dropout to use (default 0)\n",
        "    device=device).to(device)                   # put the model on the chosen gpu or cpu"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss and Optimiser"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's at this stage that we specify the loss function to use and what optimiser to use.\n",
        "\n",
        "The **optimiser** is the algorithm that is used to explore the parameter space of the network weights. In this example we're going to use `Adam` with a learning rate of 0.001 and the other parameters left as their default values. There a various different optimisers to choose from but Adam has proven to be realiable for a wide variety of problems and is a good place to start.\n",
        "\n",
        "We then need to define the function that will quantify the network's performance, the **loss function**. In this case we're using **Categorical Crossentropy**. This combined with the **Softmax layer** means the network will ouput a vector of probabilities for each samples where each probability corresponds to a particular class.\n",
        "\n",
        "For this binary (noise/signal) case it can written as:\n",
        "\n",
        "$$f(\\theta) = - \\sum_{i \\in S} log(\\theta_{i}^{S}) - \\sum_{i \\in N} log(\\theta_{i}^{N})$$\n",
        "\n",
        "where $\\theta_{i}^{S/N}$ is the predicted probability of class signal-noise (S) or noise-only (N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimiser = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_batch(model, optimiser, loss_function, data, labels, train=True):\n",
        "    \"\"\"\n",
        "    Compute the loss for one batch of training/validation data. If train=True update the model.\n",
        "    Args\n",
        "    -------\n",
        "    model: \n",
        "        pytorch model\n",
        "    optimiser:\n",
        "    loss_function:\n",
        "    data: Tensor\n",
        "    labels: Tensor\n",
        "    train: bool\n",
        "        if true updates the model weights and optimiser\n",
        "    \"\"\"\n",
        "    model.train(train)\n",
        "    if not train:\n",
        "        model.eval()\n",
        "        \n",
        "    # zero the gradients so they do not grow with time\n",
        "    if train:\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "    # pass the data through the model to get two outputs (prob noise, prob signal)\n",
        "    outputs = model(data)\n",
        "    # computs the loss from our outputs and labels\n",
        "    loss = loss_function(outputs, labels)\n",
        "\n",
        "    if train:\n",
        "        # perform a backward pass computing the gradients within the network\n",
        "        loss.backward()\n",
        "        # take a step in the direction of these gradients\n",
        "        optimiser.step()\n",
        "\n",
        "    return loss.item()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Move all the data to the GPU (if the gpu is available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "even_imgs = torch.Tensor(even_imgs).to(device)\n",
        "odd_imgs = torch.Tensor(odd_imgs).to(device)\n",
        "even_onehotlabels = even_onehotlabels.to(device)\n",
        "odd_onehotlabels = odd_onehotlabels.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now write out simple training loop. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tr_losses = np.zeros(n_epochs)\n",
        "val_losses = np.zeros(n_epochs)\n",
        "# define a random 200 indices for the validation data\n",
        "val_inds = np.random.uniform(0, len(odd_imgs), 200).astype(int)\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # train a batch on all of the \"even\" data\n",
        "    tr_loss = train_batch(cnn, optimiser, loss_fn, even_imgs[:,:], even_onehotlabels, train=True)\n",
        "\n",
        "    # dont compute gradients for the test data \n",
        "    with torch.no_grad():\n",
        "        val_loss = train_batch(cnn, optimiser, loss_fn, odd_imgs[val_inds,:], odd_onehotlabels[val_inds], train=False)\n",
        "\n",
        "    # save the training and validation losses to array to look at later\n",
        "    tr_losses[epoch] = tr_loss\n",
        "    val_losses[epoch] = val_loss\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch: {epoch}, loss: {tr_loss}, val_loss: {val_loss}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can have a look at how the loss evolves during training. From this plot it appears there is a little bit of a generalisation gap, where the validations loss and training loss diverge. As we are using such a small data set this is not surprising, the network has been simplified as much as possible to reduce this. When training for actual observing runs the data set will have > 100k examples rather than 3k which reduces this affect and improves the networks ability to generalise. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(tr_losses, label=\"Training loss\")\n",
        "ax.plot(val_losses, label=\"Validation Loss\")\n",
        "ax.set_yscale(\"log\")\n",
        "ax.set_xlabel(\"Epoch\")\n",
        "ax.set_ylabel(\"Loss\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now test this method on the \"odd\" bands to see how it performs on data it has not yet seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.eval()\n",
        "with torch.no_grad():\n",
        "    odd_outputs = cnn(torch.Tensor(odd_imgs[:,:])).cpu()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compute how well the cnn and SOAP performs by measuring the true positive rate at a false alarm of 1%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn_99per = sorted(odd_outputs[odd_labels==0][:,0])[int(0.99*len(odd_outputs[odd_labels==0]))]\n",
        "soap_99per = sorted(odd_soapouts[odd_labels==0])[int(0.99*len(odd_outputs[odd_labels==0]))]\n",
        "print(cnn_99per.numpy(), soap_99per)\n",
        "cnn_sigfrac = sum(odd_outputs[odd_labels==1][:,1] > cnn_99per)/len(odd_outputs[odd_labels==1])\n",
        "soap_sigfrac = sum(odd_soapouts[odd_labels==1] > soap_99per)/len(odd_outputs[odd_labels==1])\n",
        "print(\"cnn true positive rate: \", cnn_sigfrac.numpy())\n",
        "print(\"soap true positive rate: \", soap_sigfrac)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that it reaches close to the same sensitivity on this dataset with only a small amount of training and a small network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols = 2, figsize=(14,5))\n",
        "h11 = ax[0].hist(odd_outputs[odd_labels==0][:,1], bins=np.arange(-1,50), alpha=0.5)\n",
        "h12 = ax[0].hist(odd_outputs[odd_labels==1][:,1], bins=np.arange(-1,50), alpha=0.5)\n",
        "ax[0].axvline(cnn_99per, color=\"r\")\n",
        "ax[0].set_yscale(\"log\")\n",
        "h11 = ax[1].hist(torch.nn.functional.sigmoid(odd_outputs[odd_labels==0][:,1]), bins=30, alpha=0.5)\n",
        "h12 = ax[1].hist(torch.nn.functional.sigmoid(odd_outputs[odd_labels==1][:,1]), bins=30, alpha=0.5)\n",
        "ax[1].axvline(torch.nn.functional.sigmoid(cnn_99per), color=\"r\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading a pretrained model\n",
        "Links to other pretrained models are in the ligo private pages here if needed, I've provided an example in this repo for those who do not have access."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_soap, model_config = soapcw.cnn.pytorch.load_model_from_config(\n",
        "        config_file, \n",
        "        weight_file, \n",
        "        device=device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This can be tested on the above data (this is not an entirely fair test as the SNR distributions it was trained on are slightly different)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_soap.eval()\n",
        "with torch.no_grad():\n",
        "    s_odd_outputs = model_soap(torch.Tensor(odd_imgs)).cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols = 1, figsize=(8,5))\n",
        "bins = np.linspace(min(s_odd_outputs[:,1]), max(s_odd_outputs[:,1]), 20)\n",
        "h11 = ax.hist(s_odd_outputs[odd_labels==0][:,1], alpha=0.5, bins=bins)\n",
        "h12 = ax.hist(s_odd_outputs[odd_labels==1][:,1], alpha=0.5, bins=bins)\n",
        "#ax[0].set_yscale(\"log\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can test this network on the data we simulated at the start of this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data = torch.unsqueeze(torch.Tensor(np.stack([spect.H1.summed_norm_sft_power.T, spect.H1.summed_norm_sft_power.T, soapout.vitmap.T])), 0)\n",
        "test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    s_odd_test = model_soap(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "s_odd_test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neville "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "soapcw",
      "language": "python",
      "name": "soapcw"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
